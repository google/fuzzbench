diff --git a/Makefile b/Makefile
index 5e800db..4abc484 100644
--- a/Makefile
+++ b/Makefile
@@ -33,7 +33,7 @@ CFLAGS     += -Wall -D_FORTIFY_SOURCE=2 -g -Wno-pointer-sign \
 	      -DBIN_PATH=\"$(BIN_PATH)\"
 
 ifneq "$(filter Linux GNU%,$(shell uname))" ""
-  LDFLAGS  += -ldl
+  LDFLAGS  += -ldl -lm
 endif
 
 ifeq "$(findstring clang, $(shell $(CC) --version 2>/dev/null))" ""
diff --git a/afl-fuzz.c b/afl-fuzz.c
index 61a3ee9..7d06caa 100644
--- a/afl-fuzz.c
+++ b/afl-fuzz.c
@@ -56,6 +56,7 @@
 #include <termios.h>
 #include <dlfcn.h>
 #include <sched.h>
+#include <math.h>
 
 #include <sys/wait.h>
 #include <sys/time.h>
@@ -87,6 +88,9 @@
 #  define EXP_ST static
 #endif /* ^AFL_LIB */
 
+#define HIT_COUNTS_SIZE (1 << 21)
+u8 *hit_counts;
+
 /* Lots of globals, but mostly for the status UI and other things where it
    really makes no sense to haul them around as function parameters. */
 
@@ -159,6 +163,9 @@ static volatile u8 stop_soon,         /* Ctrl-C pressed?                  */
                    clear_screen = 1,  /* Window resized?                  */
                    child_timed_out;   /* Traced process timed out?        */
 
+EXP_ST u8 chosen_abundant,
+          focus_on_rare;
+
 EXP_ST u32 queued_paths,              /* Total number of queued testcases */
            queued_variable,           /* Testcases with variable behavior */
            queued_at_start,           /* Total number of initial inputs   */
@@ -166,6 +173,7 @@ EXP_ST u32 queued_paths,              /* Total number of queued testcases */
            queued_imported,           /* Items imported via -S            */
            queued_favored,            /* Paths deemed favorable           */
            queued_with_cov,           /* Paths with new coverage bytes    */
+           fuzzed_paths,
            pending_not_fuzzed,        /* Queued but not done yet          */
            pending_favored,           /* Pending favored paths            */
            cur_skipped_paths,         /* Abandoned inputs in cur cycle    */
@@ -247,6 +255,7 @@ struct queue_entry {
       has_new_cov,                    /* Triggers new coverage?           */
       var_behavior,                   /* Variable behavior?               */
       favored,                        /* Currently favored?               */
+      n_chosen,
       fs_redundant;                   /* Marked as redundant in the fs?   */
 
   u32 bitmap_size,                    /* Number of bits set in bitmap     */
@@ -1493,6 +1502,9 @@ static void read_testcases(void) {
 
     add_to_queue(fn, st.st_size, passed_det);
 
+    u32 cksum = hash32(trace_bits, MAP_SIZE, HASH_CONST);
+    hit_counts[cksum % HIT_COUNTS_SIZE] = 1;
+
   }
 
   free(nl); /* not tracked */
@@ -3150,6 +3162,12 @@ static u8 save_if_interesting(char** argv, void* mem, u32 len, u8 fault) {
   s32 fd;
   u8  keeping = 0, res;
 
+  u32 cksum = hash32(trace_bits, MAP_SIZE, HASH_CONST);
+
+  /* Saturated increment */
+  if (hit_counts[cksum % HIT_COUNTS_SIZE] < 0xFF)
+    hit_counts[cksum % HIT_COUNTS_SIZE]++;
+
   if (fault == crash_mode) {
 
     /* Keep only if there are new bits in the map, add to queue for
@@ -3178,7 +3196,9 @@ static u8 save_if_interesting(char** argv, void* mem, u32 len, u8 fault) {
       queued_with_cov++;
     }
 
-    queue_top->exec_cksum = hash32(trace_bits, MAP_SIZE, HASH_CONST);
+    queue_top->exec_cksum = cksum;
+
+    hit_counts[cksum % HIT_COUNTS_SIZE] = 1;
 
     /* Try to calibrate inline; this also calls update_bitmap_score() when
        successful. */
@@ -4978,22 +4998,31 @@ static u8 could_be_interest(u32 old_val, u32 new_val, u8 blen, u8 check_le) {
 
 }
 
+static u64 compute_weight(struct queue_entry *q, u32 q_index) {
 
-/* Take the current entry from the queue, fuzz it for a while. This
-   function is a tad too long... returns 0 if fuzzed successfully, 1 if
-   skipped or bailed out. */
+  if (!focus_on_rare && chosen_abundant <= 3)
+    return (q->favored ? 4 : 1 ) * q_index + 1;
 
-static u8 fuzz_one(char** argv) {
+  focus_on_rare = 1;
 
-  s32 len, fd, temp_len, i, j;
-  u8  *in_buf, *out_buf, *orig_in, *ex_tmp, *eff_map = 0;
-  u64 havoc_queued,  orig_hit_cnt, new_hit_cnt;
-  u32 splice_cycle = 0, perf_score = 100, orig_perf, prev_cksum, eff_cnt = 1;
+  u8 hits = hit_counts[q->exec_cksum % HIT_COUNTS_SIZE];
 
-  u8  ret_val = 1, doing_det = 0;
+  if (hits == 0xFF) {
 
-  u8  a_collect[MAX_AUTO_EXTRA];
-  u32 a_len = 0;
+    if (q->favored) return 4;
+    else return 1;
+
+  }
+
+  return (0x100 - hits) * (q->favored ? 4 : 1) * 4;
+
+}
+
+
+/* Take the current entry from the queue, fuzz it for a while. This
+   function is a tad too long... returns 0 if fuzzed successfully, 1 if
+   skipped or bailed out. */
+static u8 fuzz_one(char** argv) {
 
 #ifdef IGNORE_FINDS
 
@@ -5007,35 +5036,99 @@ static u8 fuzz_one(char** argv) {
   if (pending_favored) {
 
     /* If we have any favored, non-fuzzed new arrivals in the queue,
-       possibly skip to them at the expense of already-fuzzed or non-favored
+       skip to them at the expense of already-fuzzed or non-favored
        cases. */
 
-    if ((queue_cur->was_fuzzed || !queue_cur->favored) &&
-        UR(100) < SKIP_TO_NEW_PROB) return 1;
+    chosen_abundant = 0;
+    focus_on_rare = 0;
+
+    if (queue_cur->was_fuzzed || !queue_cur->favored) return 1;
+
+  } else {
+
+    u8 hits;
+    u8 has_rare_paths = 0;
+    struct queue_entry* q = queue;
+    while (q) {
+      hits = hit_counts[q->exec_cksum % HIT_COUNTS_SIZE];
+      if (hits < 0xFF) {
+        has_rare_paths = 1;
+        break;
+      }
+      q = q->next;
+    }
+
+    if (has_rare_paths) {
 
-  } else if (!dumb_mode && !queue_cur->favored && queued_paths > 10) {
+      u64 total_weight = 0;
+      u32 q_index = 0;
 
-    /* Otherwise, still possibly skip non-favored cases, albeit less often.
-       The odds of skipping stuff are higher for already-fuzzed inputs and
-       lower for never-fuzzed entries. */
+      q = queue;
+      while (q) {
 
-    if (queue_cycle > 1 && !queue_cur->was_fuzzed) {
+        q_index++;
+        total_weight += compute_weight(q, q_index);
 
-      if (UR(100) < SKIP_NFAV_NEW_PROB) return 1;
+        q = q->next;
+
+      }
+
+      if (compute_weight(queue_cur, current_entry) < UR(total_weight + 1)) return 1;
 
     } else {
 
-      if (UR(100) < SKIP_NFAV_OLD_PROB) return 1;
+      /* If there are no rare paths, fall back to vanilla AFL */
+
+      if (!queue_cur->favored && !dumb_mode && queued_paths > 10) {
+
+        /* Otherwise, still possibly skip non-favored cases, albeit less often.
+           The odds of skipping stuff are higher for already-fuzzed inputs and
+           lower for never-fuzzed entries. */
+
+        if (queue_cycle > 1 && !queue_cur->was_fuzzed) {
+
+          if (UR(100) < SKIP_NFAV_NEW_PROB) return 1;
+
+        } else {
+
+          if (UR(100) < SKIP_NFAV_OLD_PROB) return 1;
+
+        }
+
+      }
 
     }
 
-  }
+    if (hit_counts[queue_cur->exec_cksum % HIT_COUNTS_SIZE] >= 0x20)
+      chosen_abundant++;
+    else chosen_abundant = 0;
 
+  }
 #endif /* ^IGNORE_FINDS */
 
+  s32 len, fd, temp_len, i, j;
+  u8  *in_buf, *out_buf, *orig_in, *ex_tmp, *eff_map = 0;
+  u64 havoc_queued,  orig_hit_cnt, new_hit_cnt;
+  u32 splice_cycle = 0, perf_score = 100, orig_perf, prev_cksum, eff_cnt = 1;
+
+  u8  ret_val = 1, doing_det = 0;
+
+  u8  a_collect[MAX_AUTO_EXTRA];
+  u32 a_len = 0;
+
+  u32 rare = 0;
+  struct queue_entry *q = queue;
+  while (q) {
+    u8 hits = hit_counts[q->exec_cksum % HIT_COUNTS_SIZE];
+    if (hits < 0xFF) {
+      rare++;
+    }
+    q = q->next;
+  }
+
   if (not_on_tty) {
-    ACTF("Fuzzing test case #%u (%u total, %llu uniq crashes found)...",
-         current_entry, queued_paths, unique_crashes);
+    ACTF("Fuzzing test case #%u (%u total, %llu uniq crashes found, cycle=%llu, rare=%u)...",
+         current_entry, queued_paths, unique_crashes, queue_cycle, rare);
     fflush(stdout);
   }
 
@@ -7759,7 +7852,6 @@ static void save_cmdline(u32 argc, char** argv) {
 int main(int argc, char** argv) {
 
   s32 opt;
-  u64 prev_queued = 0;
   u32 sync_interval_cnt = 0, seek_to;
   u8  *extras_dir = 0;
   u8  mem_limit_given = 0;
@@ -7955,6 +8047,9 @@ int main(int argc, char** argv) {
   setup_signal_handlers();
   check_asan_opts();
 
+  /* Dynamically allocate memory for AFLFast schedules */
+  hit_counts = ck_alloc(HIT_COUNTS_SIZE * sizeof(u8));
+
   if (sync_id) fix_up_sync();
 
   if (!strcmp(in_dir, out_dir))
@@ -8052,15 +8147,19 @@ int main(int argc, char** argv) {
     if (stop_soon) goto stop_fuzzing;
   }
 
+  queue_cycle = 1;
+
+  cull_queue();
+
+  if (sync_id && getenv("AFL_IMPORT_FIRST"))
+    sync_fuzzers(use_argv);
+
   while (1) {
 
     u8 skipped_fuzz;
 
-    cull_queue();
-
     if (!queue_cur) {
 
-      queue_cycle++;
       current_entry     = 0;
       cur_skipped_paths = 0;
       queue_cur         = queue;
@@ -8071,38 +8170,29 @@ int main(int argc, char** argv) {
         queue_cur = queue_cur->next;
       }
 
-      show_stats();
-
-      if (not_on_tty) {
-        ACTF("Entering queue cycle %llu.", queue_cycle);
-        fflush(stdout);
-      }
-
-      /* If we had a full queue cycle with no new finds, try
-         recombination strategies next. */
+    }
 
-      if (queued_paths == prev_queued) {
+    skipped_fuzz = fuzz_one(use_argv);
 
-        if (use_splicing) cycles_wo_finds++; else use_splicing = 1;
+    if (!skipped_fuzz) {
 
-      } else cycles_wo_finds = 0;
+      cull_queue();
 
-      prev_queued = queued_paths;
+      if (!stop_soon && sync_id) {
 
-      if (sync_id && queue_cycle == 1 && getenv("AFL_IMPORT_FIRST"))
-        sync_fuzzers(use_argv);
+        if (!(sync_interval_cnt++ % SYNC_INTERVAL))
+          sync_fuzzers(use_argv);
 
-    }
+      }
 
-    skipped_fuzz = fuzz_one(use_argv);
+      fuzzed_paths++;
+      queue_cycle = 1 + 4 * fuzzed_paths / queued_paths;
 
-    if (!stop_soon && sync_id && !skipped_fuzz) {
-      
-      if (!(sync_interval_cnt++ % SYNC_INTERVAL))
-        sync_fuzzers(use_argv);
+      queue_cur->n_chosen++;
 
     }
 
+
     if (!stop_soon && exit_1) stop_soon = 2;
 
     if (stop_soon) break;
diff --git a/hash.h b/hash.h
index e17fc8f..55e6428 100644
--- a/hash.h
+++ b/hash.h
@@ -36,73 +36,23 @@
 
 #include "types.h"
 
-#ifdef __x86_64__
+#define XXH_INLINE_ALL
+#include "xxhash.h"
+#undef XXH_INLINE_ALL
 
-#define ROL64(_x, _r)  ((((u64)(_x)) << (_r)) | (((u64)(_x)) >> (64 - (_r))))
+#ifdef __x86_64__
 
 static inline u32 hash32(const void* key, u32 len, u32 seed) {
 
-  const u64* data = (u64*)key;
-  u64 h1 = seed ^ len;
-
-  len >>= 3;
-
-  while (len--) {
-
-    u64 k1 = *data++;
-
-    k1 *= 0x87c37b91114253d5ULL;
-    k1  = ROL64(k1, 31);
-    k1 *= 0x4cf5ad432745937fULL;
-
-    h1 ^= k1;
-    h1  = ROL64(h1, 27);
-    h1  = h1 * 5 + 0x52dce729;
-
-  }
-
-  h1 ^= h1 >> 33;
-  h1 *= 0xff51afd7ed558ccdULL;
-  h1 ^= h1 >> 33;
-  h1 *= 0xc4ceb9fe1a85ec53ULL;
-  h1 ^= h1 >> 33;
-
-  return h1;
+  return (u32) XXH64(key, len, seed);
 
 }
 
 #else 
 
-#define ROL32(_x, _r)  ((((u32)(_x)) << (_r)) | (((u32)(_x)) >> (32 - (_r))))
-
 static inline u32 hash32(const void* key, u32 len, u32 seed) {
 
-  const u32* data  = (u32*)key;
-  u32 h1 = seed ^ len;
-
-  len >>= 2;
-
-  while (len--) {
-
-    u32 k1 = *data++;
-
-    k1 *= 0xcc9e2d51;
-    k1  = ROL32(k1, 15);
-    k1 *= 0x1b873593;
-
-    h1 ^= k1;
-    h1  = ROL32(h1, 13);
-    h1  = h1 * 5 + 0xe6546b64;
-
-  }
-
-  h1 ^= h1 >> 16;
-  h1 *= 0x85ebca6b;
-  h1 ^= h1 >> 13;
-  h1 *= 0xc2b2ae35;
-  h1 ^= h1 >> 16;
-
-  return h1;
+  return (u32) XXH32(key, len, seed);
 
 }
 
